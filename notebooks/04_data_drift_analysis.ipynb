{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse du Data Drift — Credit Scoring\n",
    "\n",
    "Ce notebook realise l'analyse de la **derive des donnees (data drift)** pour notre modele de scoring credit.\n",
    "\n",
    "**Objectifs :**\n",
    "\n",
    "- Comprendre ce qu'est le data drift et pourquoi c'est critique en production\n",
    "- Analyser nos donnees avec **Evidently AI** (outil de reference pour le monitoring ML)\n",
    "- Simuler differents scenarios de drift et mesurer leur impact\n",
    "- Identifier les points de vigilance pour le monitoring en production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Qu'est-ce que le Data Drift ?\n",
    "\n",
    "### Le probleme\n",
    "\n",
    "Un modele ML est entraine sur des donnees historiques. En production, il recoit de **nouvelles donnees** pour faire ses predictions. Si ces nouvelles donnees ont des **distributions differentes** de celles d'entrainement, le modele risque de **perdre en fiabilite** — c'est le **data drift**.\n",
    "\n",
    "### Analogie concrete\n",
    "\n",
    "Imagine un modele de scoring entraine sur des clients francais avec des revenus moyens de 30 000 euros. Si en production il recoit soudainement des clients avec des revenus moyens de 80 000 euros (par exemple suite a l'ouverture du service a une clientele premium), la distribution du revenu a **derive**. Le modele n'a jamais vu ces profils — ses predictions deviennent moins fiables.\n",
    "\n",
    "### Les types de drift\n",
    "\n",
    "| Type              | Description                                          | Exemple                                            |\n",
    "| ----------------- | ---------------------------------------------------- | -------------------------------------------------- |\n",
    "| **Drift graduel** | Les distributions changent lentement au fil du temps | Inflation qui augmente les montants de credit      |\n",
    "| **Drift soudain** | Changement brutal dans les donnees                   | Nouveau segment de clientele, crise economique     |\n",
    "| **Feature shift** | Certaines variables specifiques changent             | Un fournisseur de score externe modifie son calcul |\n",
    "\n",
    "### Comment detecter le drift ?\n",
    "\n",
    "On compare la **distribution de reference** (donnees d'entrainement) avec la **distribution de production** (nouvelles donnees). Les tests statistiques les plus utilises :\n",
    "\n",
    "- **Test de Kolmogorov-Smirnov (KS)** : mesure la distance maximale entre deux distributions. Si la p-value < 0.05, on considere qu'il y a drift.\n",
    "- **Population Stability Index (PSI)** : mesure la stabilite d'une variable dans le temps.\n",
    "- **Wasserstein distance** : mesure le \"cout\" pour transformer une distribution en une autre.\n",
    "\n",
    "> **Important** : drift ne signifie pas forcement degradation du modele. Le modele peut etre robuste malgre un drift modere. Mais un drift significatif est un **signal d'alerte** qui necessite investigation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Chargement des donnees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ajouter la racine du projet au path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from evidently import Report\n",
    "from evidently.presets import DataDriftPreset\n",
    "from evidently.metrics import ValueDrift\n",
    "\n",
    "from monitoring.drift import compute_drift_report, simulate_drift\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "ARTIFACTS_DIR = Path(\"../artifacts\")\n",
    "DATA_DIR = Path(\"../data\")\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donnees de reference (entrainement) vs production (test)\n",
    "\n",
    "- **Reference** : `train_preprocessed.csv` — les donnees sur lesquelles le modele a ete entraine. C'est notre **baseline** : la distribution \"normale\" que le modele connait.\n",
    "- **Production simulee** : `test_preprocessed.csv` — les donnees de test, qui simulent ce que le modele recevrait en production. Elles proviennent du meme dataset mais n'ont pas ete vues a l'entrainement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les donnees\n",
    "reference = pd.read_csv(DATA_DIR / \"train_preprocessed.csv\")\n",
    "production = pd.read_csv(DATA_DIR / \"test_preprocessed.csv\")\n",
    "\n",
    "# Retirer les colonnes non-features\n",
    "if \"SK_ID_CURR\" in reference.columns:\n",
    "    reference = reference.drop(\"SK_ID_CURR\", axis=1)\n",
    "if \"SK_ID_CURR\" in production.columns:\n",
    "    production = production.drop(\"SK_ID_CURR\", axis=1)\n",
    "if \"TARGET\" in reference.columns:\n",
    "    reference = reference.drop(\"TARGET\", axis=1)\n",
    "\n",
    "# Garder uniquement les colonnes communes\n",
    "common_cols = sorted(set(reference.columns) & set(production.columns))\n",
    "reference = reference[common_cols]\n",
    "production = production[common_cols]\n",
    "\n",
    "print(f\"Reference  : {reference.shape[0]:,} lignes, {reference.shape[1]} features\")\n",
    "print(f\"Production : {production.shape[0]:,} lignes, {production.shape[1]} features\")\n",
    "print(f\"\\nApercu des 5 premieres features :\")\n",
    "reference.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives pour quelques features cles\n",
    "key_features = [\n",
    "    \"EXT_SOURCE_1\",\n",
    "    \"EXT_SOURCE_3\",\n",
    "    \"AMT_CREDIT\",\n",
    "    \"AMT_ANNUITY\",\n",
    "    \"DAYS_BIRTH\",\n",
    "    \"AMT_GOODS_PRICE\",\n",
    "]\n",
    "key_features = [f for f in key_features if f in common_cols]\n",
    "\n",
    "print(\"=== Stats Reference (train) ===\")\n",
    "display(reference[key_features].describe().round(3))\n",
    "\n",
    "print(\"\\n=== Stats Production (test) ===\")\n",
    "display(production[key_features].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Detection de drift avec Evidently AI\n",
    "\n",
    "**Evidently AI** est la bibliotheque de reference pour le monitoring ML. Elle genere des rapports interactifs qui comparent automatiquement les distributions de chaque feature entre la reference et la production.\n",
    "\n",
    "Le `DataDriftPreset` applique le **test de Kolmogorov-Smirnov** (pour les variables numeriques) sur chaque colonne et determine si un drift est detecte (p-value < 0.05).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Echantillonner pour la rapidite (toutes les features, 5000 lignes max)\n",
    "N_SAMPLES = 5000\n",
    "ref_sample = reference.sample(n=min(N_SAMPLES, len(reference)), random_state=42)\n",
    "prod_sample = production.sample(n=min(N_SAMPLES, len(production)), random_state=42)\n",
    "\n",
    "print(f\"Echantillon reference  : {len(ref_sample)} lignes\")\n",
    "print(f\"Echantillon production : {len(prod_sample)} lignes\")\n",
    "print(f\"Features analysees     : {len(common_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport Evidently : DataDriftPreset sur reference vs production\n",
    "drift_report = Report([DataDriftPreset()])\n",
    "snapshot = drift_report.run(ref_sample, prod_sample)\n",
    "\n",
    "# Afficher le rapport interactif\n",
    "snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation du rapport Evidently\n",
    "\n",
    "Le rapport ci-dessus montre :\n",
    "\n",
    "- **En haut** : le nombre total de features en drift vs stables\n",
    "- **Pour chaque feature** : la p-value du test KS, la decision (drift ou non), et les distributions comparees\n",
    "\n",
    "Si le rapport indique peu ou pas de drift entre train et test, c'est **normal** : les deux jeux proviennent du meme dataset Home Credit. Cela constitue notre **baseline saine**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les resultats sous forme structuree\n",
    "results_dict = snapshot.dict()\n",
    "\n",
    "# Parser les metriques de drift par feature\n",
    "drift_metrics = []\n",
    "for m in results_dict[\"metrics\"]:\n",
    "    if m[\"config\"][\"type\"] == \"evidently:metric_v2:ValueDrift\":\n",
    "        drift_metrics.append(\n",
    "            {\n",
    "                \"feature\": m[\"config\"][\"column\"],\n",
    "                \"method\": m[\"config\"][\"method\"],\n",
    "                \"p_value\": m[\"value\"],\n",
    "                \"threshold\": m[\"config\"][\"threshold\"],\n",
    "                \"drift_detected\": m[\"value\"] < m[\"config\"][\"threshold\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_drift = pd.DataFrame(drift_metrics).sort_values(\"p_value\")\n",
    "\n",
    "n_drifted = df_drift[\"drift_detected\"].sum()\n",
    "n_total = len(df_drift)\n",
    "print(\n",
    "    f\"\\nResultats Evidently : {n_drifted}/{n_total} features en drift ({n_drifted / n_total * 100:.1f}%)\"\n",
    ")\n",
    "print(f\"\\nTop 10 features avec le plus de drift (p-value la plus basse) :\")\n",
    "df_drift.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le rapport HTML pour partage\n",
    "report_path = Path(\"../monitoring/drift_report_baseline.html\")\n",
    "snapshot.save_html(str(report_path))\n",
    "print(f\"Rapport HTML sauvegarde : {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison avec notre approche manuelle (scipy KS)\n",
    "\n",
    "Notre module `monitoring/drift.py` utilise directement `scipy.stats.ks_2samp`. Verifions que les resultats sont coherents avec Evidently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notre approche manuelle\n",
    "manual_report = compute_drift_report(ref_sample, prod_sample, top_n=len(common_cols))\n",
    "\n",
    "# Comparer les deux approches\n",
    "comparison = manual_report.merge(\n",
    "    df_drift[[\"feature\", \"p_value\"]].rename(columns={\"p_value\": \"evidently_p_value\"}),\n",
    "    on=\"feature\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "comparison[\"manual_p_value\"] = comparison[\"p_value\"]\n",
    "comparison[\"p_value_diff\"] = abs(\n",
    "    comparison[\"manual_p_value\"] - comparison[\"evidently_p_value\"]\n",
    ")\n",
    "\n",
    "print(\"Comparaison des p-values (scipy vs Evidently) :\")\n",
    "print(\n",
    "    f\"Correlation : {comparison['manual_p_value'].corr(comparison['evidently_p_value']):.4f}\"\n",
    ")\n",
    "print(f\"Difference moyenne : {comparison['p_value_diff'].mean():.6f}\")\n",
    "print(\"\\n=> Les deux approches donnent des resultats quasi identiques.\")\n",
    "print(\"   Evidently utilise le meme test KS en interne, avec un rapport plus riche.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Simulation de scenarios de drift\n",
    "\n",
    "Pour comprendre comment le drift se manifeste, nous allons **simuler** 3 scenarios sur nos donnees de production et observer comment Evidently les detecte.\n",
    "\n",
    "Les simulations sont realisees par notre module `monitoring/drift.py` :\n",
    "\n",
    "- **Drift graduel** : ajout de bruit gaussien sur toutes les features\n",
    "- **Drift soudain** : decalage brutal des 20 premieres features\n",
    "- **Feature shift** : multiplication de 10% des features aleatoires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    {\"name\": \"Aucun drift\", \"drift_type\": \"none\", \"intensity\": 0.0},\n",
    "    {\"name\": \"Drift graduel (faible)\", \"drift_type\": \"gradual\", \"intensity\": 0.1},\n",
    "    {\"name\": \"Drift graduel (fort)\", \"drift_type\": \"gradual\", \"intensity\": 0.5},\n",
    "    {\"name\": \"Drift soudain\", \"drift_type\": \"sudden\", \"intensity\": 0.3},\n",
    "    {\"name\": \"Feature shift\", \"drift_type\": \"feature_shift\", \"intensity\": 0.3},\n",
    "]\n",
    "\n",
    "scenario_results = []\n",
    "\n",
    "for sc in scenarios:\n",
    "    # Simuler le drift sur les donnees de production\n",
    "    drifted = simulate_drift(\n",
    "        prod_sample, drift_type=sc[\"drift_type\"], intensity=sc[\"intensity\"]\n",
    "    )\n",
    "\n",
    "    # Lancer Evidently\n",
    "    report = Report([DataDriftPreset()])\n",
    "    snap = report.run(ref_sample, drifted)\n",
    "    results = snap.dict()\n",
    "\n",
    "    # Compter les features en drift\n",
    "    n_drifted = sum(\n",
    "        1\n",
    "        for m in results[\"metrics\"]\n",
    "        if m[\"config\"][\"type\"] == \"evidently:metric_v2:ValueDrift\"\n",
    "        and m[\"value\"] < m[\"config\"][\"threshold\"]\n",
    "    )\n",
    "    n_features = sum(\n",
    "        1\n",
    "        for m in results[\"metrics\"]\n",
    "        if m[\"config\"][\"type\"] == \"evidently:metric_v2:ValueDrift\"\n",
    "    )\n",
    "\n",
    "    scenario_results.append(\n",
    "        {\n",
    "            \"Scenario\": sc[\"name\"],\n",
    "            \"Type\": sc[\"drift_type\"],\n",
    "            \"Intensite\": sc[\"intensity\"],\n",
    "            \"Features en drift\": n_drifted,\n",
    "            \"Total features\": n_features,\n",
    "            \"% drift\": f\"{n_drifted / n_features * 100:.0f}%\"\n",
    "            if n_features > 0\n",
    "            else \"N/A\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_scenarios = pd.DataFrame(scenario_results)\n",
    "print(\"Resultats de la simulation de drift :\\n\")\n",
    "df_scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- **Aucun drift** : peu ou pas de features detectees — le test confirme que les donnees sont similaires.\n",
    "- **Drift graduel faible** : quelques features commencent a deriver — signal d'alerte precoce.\n",
    "- **Drift graduel fort** : une majorite de features driftent — le modele est probablement degrade.\n",
    "- **Drift soudain** : les 20 premieres features sont fortement decalees — scenario de crise.\n",
    "- **Feature shift** : ~10% des features changent — probleme cible (ex: un fournisseur de donnees modifie son calcul).\n",
    "\n",
    "Cela montre que la detection est **proportionnelle a l'intensite du drift**, ce qui est le comportement attendu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser le rapport Evidently pour le scenario \"drift soudain\"\n",
    "drifted_sudden = simulate_drift(prod_sample, drift_type=\"sudden\", intensity=0.3)\n",
    "report_sudden = Report([DataDriftPreset()])\n",
    "snap_sudden = report_sudden.run(ref_sample, drifted_sudden)\n",
    "\n",
    "# Sauvegarder pour reference\n",
    "snap_sudden.save_html(str(Path(\"../monitoring/drift_report_sudden.html\")))\n",
    "print(\"Rapport 'drift soudain' sauvegarde.\")\n",
    "\n",
    "# Afficher le rapport interactif\n",
    "snap_sudden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Analyse approfondie des features critiques\n",
    "\n",
    "Toutes les features n'ont pas la meme importance pour le modele. Les **top features** (celles qui influencent le plus les predictions) meritent une surveillance prioritaire.\n",
    "\n",
    "Voici les 10 features les plus importantes de notre modele LightGBM :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modele et extraire les importances\n",
    "model = joblib.load(ARTIFACTS_DIR / \"model.pkl\")\n",
    "with open(ARTIFACTS_DIR / \"feature_names.json\") as f:\n",
    "    feature_names = json.load(f)\n",
    "\n",
    "importances = sorted(\n",
    "    zip(feature_names, model.feature_importances_),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "top_features = [name for name, _ in importances[:10]]\n",
    "# Garder celles presentes dans les donnees\n",
    "top_features = [f for f in top_features if f in common_cols]\n",
    "\n",
    "print(\"Top features du modele (par importance) :\\n\")\n",
    "for i, (name, imp) in enumerate(importances[:10], 1):\n",
    "    flag = \" ← critique\" if i <= 3 else \"\"\n",
    "    print(f\"  {i:2d}. {name:<35s} importance = {imp}{flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de drift feature par feature avec Evidently\n",
    "top_available = [f for f in top_features if f in ref_sample.columns]\n",
    "\n",
    "report_top = Report([ValueDrift(column=col) for col in top_available])\n",
    "snap_top = report_top.run(ref_sample, prod_sample)\n",
    "\n",
    "# Extraire les resultats\n",
    "top_results = snap_top.dict()\n",
    "top_drift_data = []\n",
    "for m in top_results[\"metrics\"]:\n",
    "    if m[\"config\"][\"type\"] == \"evidently:metric_v2:ValueDrift\":\n",
    "        top_drift_data.append(\n",
    "            {\n",
    "                \"Feature\": m[\"config\"][\"column\"],\n",
    "                \"Methode\": m[\"config\"][\"method\"],\n",
    "                \"P-value\": f\"{m['value']:.6f}\",\n",
    "                \"Drift detecte\": \"OUI\"\n",
    "                if m[\"value\"] < m[\"config\"][\"threshold\"]\n",
    "                else \"NON\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_top_drift = pd.DataFrame(top_drift_data)\n",
    "print(\"Analyse de drift sur les features les plus importantes du modele :\\n\")\n",
    "df_top_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les distributions des top 3 features\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, min(3, len(top_available)), figsize=(16, 4))\n",
    "if len(top_available) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, feat in zip(axes, top_available[:3]):\n",
    "    ax.hist(\n",
    "        ref_sample[feat].dropna(),\n",
    "        bins=50,\n",
    "        alpha=0.6,\n",
    "        label=\"Reference\",\n",
    "        color=\"#1D6A4B\",\n",
    "        density=True,\n",
    "    )\n",
    "    ax.hist(\n",
    "        prod_sample[feat].dropna(),\n",
    "        bins=50,\n",
    "        alpha=0.6,\n",
    "        label=\"Production\",\n",
    "        color=\"#8B2D2D\",\n",
    "        density=True,\n",
    "    )\n",
    "    ax.set_title(feat, fontsize=12, fontweight=\"bold\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_ylabel(\"Densite\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Distributions comparees — Top Features (Reference vs Production)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation metier\n",
    "\n",
    "- **EXT_SOURCE_MEAN / EXT_SOURCE_1 / EXT_SOURCE_3** : ces features sont les **scores externes** fournis par des organismes tiers. Si elles driftent en production, cela peut signifier que le fournisseur a modifie son calcul de score. C'est un signal critique.\n",
    "\n",
    "- **AMT_CREDIT / AMT_ANNUITY / AMT_GOODS_PRICE** : montants financiers. Un drift ici peut indiquer un changement de segment de clientele (clients plus riches ou plus modestes) ou une evolution economique (inflation).\n",
    "\n",
    "- **DAYS_BIRTH** : age des clients. Un drift signifierait un changement de la demographie ciblee.\n",
    "\n",
    "- **PREV_CNT_PAYMENT_MEAN / BUREAU_DAYS_CREDIT_MAX** : historique de credit. Ces features dependent du comportement passe des clients et sont naturellement plus stables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impact du drift simule sur les features critiques\n",
    "print(\"Impact du drift graduel (intensite 0.3) sur les top features :\\n\")\n",
    "\n",
    "drifted_gradual = simulate_drift(prod_sample, drift_type=\"gradual\", intensity=0.3)\n",
    "\n",
    "report_impact = Report([ValueDrift(column=col) for col in top_available])\n",
    "snap_impact = report_impact.run(ref_sample, drifted_gradual)\n",
    "\n",
    "impact_results = snap_impact.dict()\n",
    "for m in impact_results[\"metrics\"]:\n",
    "    if m[\"config\"][\"type\"] == \"evidently:metric_v2:ValueDrift\":\n",
    "        col = m[\"config\"][\"column\"]\n",
    "        pval = m[\"value\"]\n",
    "        drifted = pval < m[\"config\"][\"threshold\"]\n",
    "        status = \"DRIFT\" if drifted else \"OK\"\n",
    "        print(f\"  {col:<35s} p-value = {pval:.6f}  [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Points de vigilance et recommandations\n",
    "\n",
    "### Resultats de l'analyse\n",
    "\n",
    "| Aspect                             | Constat                                                                                                |\n",
    "| ---------------------------------- | ------------------------------------------------------------------------------------------------------ |\n",
    "| **Drift baseline (train vs test)** | Faible ou absent — les deux jeux proviennent de la meme source, ce qui est rassurant                   |\n",
    "| **Robustesse de la detection**     | Evidently detecte correctement les drifts simules, proportionnellement a leur intensite                |\n",
    "| **Features critiques**             | EXT*SOURCE*\\*, AMT_CREDIT, DAYS_BIRTH sont les plus importantes et doivent etre monitorees en priorite |\n",
    "\n",
    "### Recommandations pour la production\n",
    "\n",
    "1. **Monitoring regulier** : executer un rapport Evidently chaque semaine (ou chaque lot de N predictions) en comparant les nouvelles donnees a la reference.\n",
    "\n",
    "2. **Seuils d'alerte** :\n",
    "   - Si < 10% des features driftent → **surveillance normale**\n",
    "   - Si 10-30% des features driftent → **alerte moderee**, investigation necessaire\n",
    "   - Si > 30% des features driftent → **alerte critique**, envisager un re-entrainement\n",
    "\n",
    "3. **Focus sur les features critiques** : meme si le drift global est faible, un drift sur EXT_SOURCE_MEAN ou AMT_CREDIT (top features) doit declencher une investigation.\n",
    "\n",
    "4. **Logging des features** : actuellement l'API logue les predictions (probabilite, decision, latence) mais pas les features d'entree. Pour un monitoring complet, il faudrait aussi logger un echantillon des features.\n",
    "\n",
    "5. **Re-entrainement** : si un drift significatif est confirme sur les features critiques ET que les metriques de performance se degradent, re-entrainer le modele avec des donnees recentes.\n",
    "\n",
    "### Conformite RGPD\n",
    "\n",
    "- Les donnees utilisees sont des **features anonymisees** (pas de noms, adresses, etc.)\n",
    "- Les logs de prediction ne contiennent que l'ID client et le score, pas les features brutes\n",
    "- Les rapports de drift analysent des **distributions statistiques**, pas des donnees individuelles\n",
    "- En production, s'assurer que les donnees de reference sont stockees de maniere securisee et avec une duree de retention definie\n",
    "\n",
    "### Limites\n",
    "\n",
    "- **Drift ≠ degradation** : un drift statistique ne signifie pas forcement que le modele performe moins bien. Les deux doivent etre surveilles conjointement.\n",
    "- **Sensibilite au sample size** : avec de grands echantillons, le test KS peut detecter des differences minimes mais statistiquement significatives, sans impact reel sur le modele.\n",
    "- **Drift simule vs reel** : nos simulations sont artificielles. En production, le drift est souvent plus subtil et multidimensionnel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyse de drift terminee.\")\n",
    "print(\"\\nFichiers generes :\")\n",
    "print(\"  - monitoring/drift_report_baseline.html  (rapport reference vs production)\")\n",
    "print(\"  - monitoring/drift_report_sudden.html     (rapport drift soudain simule)\")\n",
    "print(\n",
    "    \"\\nOuvrez ces fichiers HTML dans un navigateur pour explorer les rapports interactifs.\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
